{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation and hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives of Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand what priors, likelihoods and posteriors are;\n",
    "2. Use random sampling for parameter estimation to appreciate the relationship between sample size & the posterior distribution, along with the effect of the prior;\n",
    "3. Use probabilistic programming for parameter estimation;\n",
    "4. Use probabilistic programming for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Bayes Theorem to Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we flip a biased coin several times and we want to estimate the probability of heads from the number of heads we saw. Statistical intuition tells us that our best estimate of $p(heads)=$ number of heads divided by total number of flips.\n",
    "\n",
    "However, \n",
    "\n",
    "1. It doesn't tell us how certain we can be of that estimate and\n",
    "2. This type of intuition doesn't extend to even slightly more complex examples.\n",
    "\n",
    "Bayesian inference helps us here. We can calculate the probability of a particular $p=p(H)$ given data $D$ by setting $A$ in Bayes Theorem equal to $p$ and $B$ equal to $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$P(p|D) = \\frac{P(D|p)P(p)}{P(D)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, we call $P(p)$ the prior (distribution), $P(D|p)$ the likelihood and $P(p|D)$ the posterior (distribution). The intuition behind the nomenclature is as follows: the prior is the distribution containing our knowledge about $p$ prior to the introduction of the data $D$ & the posterior is the distribution containing our knowledge about $p$ after considering the data $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we're _overloading_ the term _probability_ here. In fact, we have 3 distinct usages of the word:\n",
    "- The probability $p$ of seeing a head when flipping a coin;\n",
    "- The resulting binomial probability distribution $P(D|p)$ of seeing the data $D$, given $p$;\n",
    "- The prior & posterior probability distributions of $p$, encoding our _uncertainty_ about the value of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key concept:** We only need to know the posterior distribution $P(p|D)$ up to multiplication by a constant at the moment: this is because we really only care about the values of $P(p|D)$ relative to each other – for example, what is the most likely value of $p$? To answer such questions, we only need to know what $P(p|D)$ is proportional to, as a function of $p$. Thus we don’t currently need to worry about the term $P(D)$. In fact,\n",
    "\n",
    "$$P(p|D) \\propto P(D|p)P(p) $$\n",
    "\n",
    "**Note:** What is the prior? Really, what do we know about $p$ before we see any data? Well, as it is a probability, we know that $0\\leq p \\leq1$. If we haven’t flipped any coins yet, we don’t know much else: so it seems logical that all values of $p$ within this interval are equally likely, i.e., $P(p)=1$, for $0\\leq p \\leq1$. This is known as an uninformative prior because it contains little information (there are other uninformative priors we may use in this situation, such as the Jeffreys prior, to be discussed later). People who like to hate on Bayesian inference tend to claim that the need to choose a prior makes Bayesian methods somewhat arbitrary, but as we’ll now see, if you have enough data, the likelihood dominates over the prior and the latter doesn’t matter so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Essential remark:** we get the whole distribution of $P(p|D)$, not merely a point estimate plus errors bars, such as [95% confidence intervals](http://andrewgelman.com/2018/07/04/4th-july-lets-declare-independence-95/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian parameter estimation I: flip those coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some coin flips and try to estimate $p(H)$. Two notes:\n",
    "- given data $D$ consisting of $n$ coin tosses & $k$ heads, the likelihood function is given by $L:=P(D|p) \\propto p^k(1-p)^k$;\n",
    "- given a uniform prior, the posterior is proportional to the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(p=0.6, N=0):\n",
    "    \"\"\"Plot the posterior given a uniform prior; Coin flips\n",
    "    with probability p; sample size N\"\"\"\n",
    "    # Set seed\n",
    "    np.random.seed(42)\n",
    "    # Flip coins \n",
    "    n_successes = np.random.binomial(N,p)\n",
    "    # X-axis for PDF\n",
    "    x = np.linspace(0,1,100)\n",
    "    #prior\n",
    "    prior = 1\n",
    "    posterior = x**n_successes*(1-x)**(N-n_successes)*prior\n",
    "    posterior /= np.max(posterior)  # so that peak always at 1\n",
    "    plt.plot(x, posterior)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot posterior for 10 coin flips\n",
    "plot_posterior(N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now use the great ipywidget interact to check out the posterior as you generate more and more data (you can also vary $p$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96be3fa946b243648984782fc937a105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.6, description='p', max=1.0, step=0.01), IntSlider(value=0, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_posterior(p=0.6, N=0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_posterior, p=(0,1, 0.01), N=(0,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes for discussion:**\n",
    "\n",
    "* as you generate more and more data, your posterior gets narrower, i.e. you get more and more certain of your estimate.\n",
    "* you need more data to be certain of your estimate when $p=0.5$, as opposed to when $p=0$ or $p=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we needed to choose a prior and that, in the small to medium data limit, this choice can affect the posterior. We'll briefly introduce several types of priors and then you'll use one of them for the example above to see the effect of the prior:\n",
    "\n",
    "- **Informative priors** express specific, definite information about a variable, for example, if we got a coin from the mint, we may use an informative prior with a peak at $p=0.5$ and small variance. \n",
    "- **Weakly informative priors** express partial information about a variable, such as a peak at $p=0.5$ (if we have no reason to believe the coin is biased), with a larger variance.\n",
    "- **Uninformative priors** express no information about a variable, except what we know for sure, such as knowing that $0\\leq p \\leq1$.\n",
    "\n",
    "Now you may think that the _uniform distribution_ is uninformative, however, what if I am thinking about this question in terms of the probability $p$ and Eric Ma is thinking about it in terms of the _odds ratio_ $r=\\frac{p}{1-p}$? Eric rightly feels that he has no prior knowledge as to what this $r$ is and thus chooses the uniform prior on $r$.\n",
    "\n",
    "With a bit of algebra (transformation of variables), we can show that choosing the uniform prior on $p$ amounts to choosing a decidedly non-uniform prior on $r$ and vice versa. So Eric and I have actually chosen different priors, using the same philosophy. How do we avoid this happening? Enter the **Jeffreys prior**, which is an uninformative prior that solves this problem. You can read more about the Jeffreys prior [here](https://en.wikipedia.org/wiki/Jeffreys_prior) & in your favourite Bayesian text book (Sivia gives a nice treatment). \n",
    "\n",
    "In the binomial (coin flip) case, the Jeffreys prior is given by $P(p) = \\frac{1}{\\sqrt{p(1-p)}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an interactive plot like the one above, except that it has two posteriors on it: one for the uniform prior, another for the Jeffries prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the plotting function, as above\n",
    "# Solution\n",
    "def plot_posteriors(p=0.6, N=0):\n",
    "    np.random.seed(42)\n",
    "    n_successes = np.random.binomial(N, p)\n",
    "    x = np.linspace(0.01, 0.99, 100)\n",
    "    posterior1 = x**n_successes*(1-x)**(N-n_successes)  # w/ uniform prior\n",
    "    posterior1 /= np.max(posterior1)  # so that peak always at 1\n",
    "    plt.plot(x, posterior1, label='Uniform prior')\n",
    "    jp = np.sqrt(x*(1-x))**(-1)  # Jeffreys prior\n",
    "    posterior2 = posterior1*jp  # w/ Jeffreys prior\n",
    "    posterior2 /= np.max(posterior2)  # so that peak always at 1 (not quite correct to do; see below)\n",
    "    plt.plot(x, posterior2, label='Jeffreys prior')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0d6d945e3147cfadf30242f4a0e501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.6, description='p', max=1.0, step=0.01), IntSlider(value=0, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_posteriors(p=0.6, N=0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the interactive plot\n",
    "interact(plot_posteriors, p=(0,1, 0.01), N=(0,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What happens to the posteriors as you generate more and more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian parameter estimation using PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You've learnt the basics of Bayesian model building. The steps are\n",
    "1. To completely specify the model in terms of _probability distributions_. This includes specifying \n",
    "    - what the form of the sampling distribution of the data is _and_ \n",
    "    - what form describes our _uncertainty_ in the unknown parameters (This formulation is adapted from [Fonnesbeck's workshop](https://github.com/fonnesbeck/intro_stat_modeling_2017/blob/master/notebooks/2.%20Basic%20Bayesian%20Inference.ipynb) as Chris said it so well there).\n",
    "2. Calculate the _posterior distribution_.\n",
    "\n",
    "In the above, the form of the sampling distribution of the data was Binomial (described by the likelihood) and the uncertainty around the unknown parameter $p$ captured by the prior.\n",
    "\n",
    "There is also a 3rd step that Eric will cover which is performing _posterior predictive checks_. At a high level, this is making sure that your model actually makes sense. More to come on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to do the same using the **probabilistic programming language** PyMC3. There's _loads_ about PyMC3 and this paradigm, two of which are\n",
    "- _probabililty distributions_ are first class citizens, in that we can assign them to variables and use them intuitively to mirror how we think about priors, likelihoods & posteriors.\n",
    "- PyMC3 calculates the posterior for us!\n",
    "\n",
    "Under the hood, PyMC3 will compute the posterior using a sampling based approach called Markov Chain Monte Carlo (MCMC) or Variational Inference. Check the [PyMC3 docs](https://docs.pymc.io/) for more on these. \n",
    "\n",
    "But now, it's time to bust out some MCMC and get sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation I: click-through rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common experiment in tech data science is to test a product change and see how it affects a metric that you're interested in. Say that I don't think enough people are clicking a button on my website & I hypothesize that it's because the button is a similar color to the background of the page. Then I can set up two pages and send some people to each: the first the original page, the second a page that is identical, except that it has a button that is of higher contrast and see if more people click through. This is commonly referred to as an A/B test and the metric of interest is click-through rate (CTR), what proportion of people click through. Before even looking at two rates, let's use PyMC3 to estimate one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First generate click-through data, given a CTR $p_a=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "p_a = 0.15\n",
    "N = 150\n",
    "n_successes_a = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build your probability model. Noticing that our model of having a constant CTR resulting in click or not is a biased coin flip,\n",
    "- the sampling distribution is binomial and we need to encode this in the likelihood;\n",
    "- there is a single parameter $p$ that we need to describe the uncertainty around, using a prior and we'll use a uniform prior for this.\n",
    "\n",
    "These are the ingredients for the model so let's now build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model of p_a\n",
    "with ___ as ___:\n",
    "    # Prior on p\n",
    "    prob = ___\n",
    "    # Binomial Likelihood\n",
    "    y = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \n",
    "- What do you think of the API for PyMC3. Does it reflect how we think about model building?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to sample from the posterior using PyMC3. You'll also plot the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from posterior\n",
    "with ___:\n",
    "    samples = ___\n",
    "\n",
    "# Plot posterior\n",
    "pm.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For discussion:** Interpret the posterior ditribution. What would your tell the non-technical manager of your growth team about the CTR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Parameter estimation II -- the mean of a population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll calculate the  posterior mean beak depth of Galapagos finches in a given species. First you'll load the data and subset wrt species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and subset it\n",
    "df_12 = pd.read_csv('../data/finch_beaks_2012.csv')\n",
    "df_fortis = df_12.loc[df_12['species'] == 'fortis']\n",
    "df_scandens = df_12.loc[df_12['species'] == 'scandens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the full probabilty model, you need\n",
    "- a likelihood function for the data &\n",
    "- priors for all unknowns.\n",
    "\n",
    "What is the likelihood here? Let's plot the measurements below and see that they look approximately Gaussian/normal so you'll use a normal likelihood $y_i\\sim \\mathcal{N}(\\mu, \\sigma^2)$. The unknowns here are the mean $\\mu$ and standard deviation $\\sigma$ and we'll use weakly informative priors on both\n",
    "- a normal prior for $\\mu$ with mean $10$ and standard deviation $5$;\n",
    "- a uniform prior for $\\sigma$ bounded between $0$ and $10$.\n",
    "\n",
    "We can discuss biological reasons for these priors also but you can also test that the posteriors are relativelyt robust to the choice of prior here due to the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_fortis['blength']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ___ as ___:\n",
    "    # Prior for mean & standard deviation\n",
    "    μ_1 = ___\n",
    "    σ_1 = ___\n",
    "    # Gaussian Likelihood\n",
    "    y_1 = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bust it out & sample\n",
    "with model:\n",
    "    samples = ___\n",
    "\n",
    "# Plot posterior\n",
    "pm.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hypothesis testing I: A/B tests on click through rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a website and want to redesign the layout *A* and test whether the new layout *B* results in a higher click through rate. When people come to our website we randomly show them layout *A* or *B* and see how many people click through for each. First let's generate the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click-through rates\n",
    "p_a = 0.15\n",
    "p_b = 0.20\n",
    "N = 1000\n",
    "n_successes_a = np.sum(np.random.uniform(size=N) <= p_a)\n",
    "n_successes_b = np.sum(np.random.uniform(size=N) <= p_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to specify our models for $p_a$ and $p_b$. Each will be the same as the CTR example above\n",
    "- Binomial likelihoods\n",
    "- uniform priors on $p_a$ and $_p$.\n",
    "\n",
    "We also want to calculate the posterior of the difference $p_a-p_b$ and we do so using `pm.Deterministic()`, which specifies a deterministic random variable, i.e., one that is completely determined by the values it references, in the case $p_a$ & $p_b$.\n",
    "\n",
    "We'll now build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ___ as ___:\n",
    "    # Prior on p\n",
    "    prob_a = ___\n",
    "    prob_b = ___\n",
    "    # Binomial Likelihood\n",
    "    y_a = ___\n",
    "    y_b = ___\n",
    "    diff_clicks = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the posterior and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model:\n",
    "    samples = pm.sample(2000, njobs=1)\n",
    "pm.plot_posterior(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Bayesian Hypothesis testing II -- beak lengths difference between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Determine whether the mean beak length of the Galapogas finches differs between species. For the mean of each species, use the same model as in previous hand-on section:\n",
    "\n",
    "- Gaussian likelihood;\n",
    "- Normal prior for the means;\n",
    "- Uniform prior for the variances.\n",
    "\n",
    "Also calculate the difference between the means and, for bonus points, the _effect size_, which is the difference between the means divided by the pooled standard deviations = $\\sqrt{(\\sigma_1^2+\\sigma_2^2)/2}$. Hugo will talk through the importance of the _effect size_.\n",
    "\n",
    "Don't forget to sample from the posteriors and plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors for means and variances\n",
    "    μ_1 = ___\n",
    "    σ_1 = ___\n",
    "    μ_2 = ___\n",
    "    σ_2 = ___\n",
    "    # Gaussian Likelihoods\n",
    "    y_1 = pm.Normal('y_1', mu=μ_1, sd=σ_1, observed=df_fortis['blength'])\n",
    "    y_2 = pm.Normal('y_2', mu=μ_2, sd=σ_2, observed=df_scandens['blength'])\n",
    "    # Calculate the effect size and its uncertainty.\n",
    "    diff_means = ___\n",
    "    pooled_sd = ___\n",
    "    effect_size = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bust it out & sample\n",
    "with model:\n",
    "    samples = pm.sample(2000, njobs=1)\n",
    "pm.plot_posterior(samples, varnames=['μ_1', 'μ_2', 'diff_means', 'effect_size']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bayesian-modelling-tutorial]",
   "language": "python",
   "name": "conda-env-bayesian-modelling-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
